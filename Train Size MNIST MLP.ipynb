{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBasic example originally from https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py\\n\\nTrains a simple deep NN on the MNIST dataset.\\n\\nGets to 98.40% test accuracy after 20 epochs\\n(there is *a lot* of margin for parameter tuning).\\n2 seconds per epoch on a K520 GPU.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Basic example originally from https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "Trains a simple deep NN on the MNIST dataset.\n",
    "\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout \n",
    "from keras.optimizers import RMSprop\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 Total train samples\n",
      "10000 Total test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'Total train samples')\n",
    "print(x_test.shape[0], 'Total test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(sample_count):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#     model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "#     random_indeces = np.arange(len(x_train))\n",
    "#     np.random.shuffle(random_indeces)\n",
    "#     x_train = x[random_indeces]\n",
    "#     y_train = y[random_indeces]\n",
    "\n",
    "    x_train_small = x_train[:][:sample_count]\n",
    "    y_train_small = y_train[:][:sample_count]\n",
    "    print('Training Shape:', x_train_small.shape)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    training = model.fit(x_train_small, y_train_small,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    training.history['acc'] = 100.0 * np.array(training.history['acc']) # Convert to percent\n",
    "    training.history['val_acc'] = 100.0 * np.array(training.history['val_acc']) # Convert to percent\n",
    "    \n",
    "    # Start the accuracy at 10% before training started\n",
    "    training.history['acc'] = np.insert(training.history['acc'], 0, 10.0)\n",
    "    training.history['val_acc'] = np.insert(training.history['val_acc'], 0, 10.0)\n",
    "    \n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (60, 784)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 2.3162 - acc: 0.1167 - val_loss: 1.9670 - val_acc: 0.3713\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 1.3887 - acc: 0.5833 - val_loss: 1.6338 - val_acc: 0.5159\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.9784 - acc: 0.8833 - val_loss: 1.9004 - val_acc: 0.3892\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.7273 - acc: 0.7833 - val_loss: 1.3772 - val_acc: 0.5870\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5542 - acc: 0.9000 - val_loss: 1.2908 - val_acc: 0.6012\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.2276 - acc: 1.0000 - val_loss: 1.0630 - val_acc: 0.6605\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.1298 - acc: 1.0000 - val_loss: 1.1074 - val_acc: 0.6525\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.0760 - acc: 1.0000 - val_loss: 1.0276 - val_acc: 0.6740\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.0530 - acc: 1.0000 - val_loss: 1.0419 - val_acc: 0.6691\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0402 - acc: 1.0000 - val_loss: 1.0387 - val_acc: 0.6701\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0321 - acc: 1.0000 - val_loss: 1.0375 - val_acc: 0.6709\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.0264 - acc: 1.0000 - val_loss: 1.0395 - val_acc: 0.6707\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.0221 - acc: 1.0000 - val_loss: 1.0398 - val_acc: 0.6730\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0188 - acc: 1.0000 - val_loss: 1.0423 - val_acc: 0.6725\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0161 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 0.6747\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0140 - acc: 1.0000 - val_loss: 1.0474 - val_acc: 0.6736\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 1.0495 - val_acc: 0.6745\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0108 - acc: 1.0000 - val_loss: 1.0543 - val_acc: 0.6745\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.6749\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 1.0604 - val_acc: 0.6753\n",
      "Test loss: 1.060397772884369\n",
      "Test accuracy: 0.6753\n",
      "Training Shape: (600, 784)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 600 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "600/600 [==============================] - 1s 2ms/step - loss: 1.5962 - acc: 0.5050 - val_loss: 1.1061 - val_acc: 0.6386\n",
      "Epoch 2/20\n",
      "600/600 [==============================] - 0s 766us/step - loss: 0.6911 - acc: 0.8083 - val_loss: 0.8615 - val_acc: 0.7258\n",
      "Epoch 3/20\n",
      "600/600 [==============================] - 0s 776us/step - loss: 0.3871 - acc: 0.8917 - val_loss: 0.5453 - val_acc: 0.8261\n",
      "Epoch 4/20\n",
      "600/600 [==============================] - 0s 800us/step - loss: 0.2630 - acc: 0.9333 - val_loss: 0.5584 - val_acc: 0.8288\n",
      "Epoch 5/20\n",
      "600/600 [==============================] - 0s 782us/step - loss: 0.1539 - acc: 0.9733 - val_loss: 0.5775 - val_acc: 0.8165\n",
      "Epoch 6/20\n",
      "600/600 [==============================] - 1s 862us/step - loss: 0.1641 - acc: 0.9517 - val_loss: 0.4489 - val_acc: 0.8636\n",
      "Epoch 7/20\n",
      "600/600 [==============================] - 0s 709us/step - loss: 0.0745 - acc: 0.9950 - val_loss: 0.4431 - val_acc: 0.8649\n",
      "Epoch 8/20\n",
      "600/600 [==============================] - 0s 607us/step - loss: 0.0501 - acc: 0.9967 - val_loss: 0.4926 - val_acc: 0.8516\n",
      "Epoch 9/20\n",
      "600/600 [==============================] - 0s 700us/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.4822 - val_acc: 0.8567\n",
      "Epoch 10/20\n",
      "600/600 [==============================] - 0s 618us/step - loss: 0.2106 - acc: 0.9567 - val_loss: 0.5353 - val_acc: 0.8560\n",
      "Epoch 11/20\n",
      "600/600 [==============================] - 0s 650us/step - loss: 0.0310 - acc: 0.9983 - val_loss: 0.4208 - val_acc: 0.8747\n",
      "Epoch 12/20\n",
      "600/600 [==============================] - 0s 614us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.4302 - val_acc: 0.8770\n",
      "Epoch 13/20\n",
      "600/600 [==============================] - 0s 548us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.4279 - val_acc: 0.8767\n",
      "Epoch 14/20\n",
      "600/600 [==============================] - 0s 578us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.4402 - val_acc: 0.8799\n",
      "Epoch 15/20\n",
      "600/600 [==============================] - 0s 586us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.4452 - val_acc: 0.8811\n",
      "Epoch 16/20\n",
      "600/600 [==============================] - 0s 588us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.4629 - val_acc: 0.8788\n",
      "Epoch 17/20\n",
      "600/600 [==============================] - 0s 548us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.4629 - val_acc: 0.8756\n",
      "Epoch 18/20\n",
      "600/600 [==============================] - 0s 549us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.4912 - val_acc: 0.8734\n",
      "Epoch 19/20\n",
      "600/600 [==============================] - 0s 549us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.4916 - val_acc: 0.8777\n",
      "Epoch 20/20\n",
      "600/600 [==============================] - 0s 543us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.4983 - val_acc: 0.8802\n",
      "Test loss: 0.49832952905287964\n",
      "Test accuracy: 0.8802\n",
      "Training Shape: (6000, 784)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "6000/6000 [==============================] - 1s 208us/step - loss: 0.6084 - acc: 0.8143 - val_loss: 0.3901 - val_acc: 0.8741\n",
      "Epoch 2/20\n",
      "6000/6000 [==============================] - 1s 189us/step - loss: 0.2439 - acc: 0.9280 - val_loss: 0.2804 - val_acc: 0.9106\n",
      "Epoch 3/20\n",
      "6000/6000 [==============================] - 1s 175us/step - loss: 0.1493 - acc: 0.9557 - val_loss: 0.2360 - val_acc: 0.9252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "6000/6000 [==============================] - 1s 177us/step - loss: 0.1023 - acc: 0.9688 - val_loss: 0.2216 - val_acc: 0.9350\n",
      "Epoch 5/20\n",
      "6000/6000 [==============================] - 1s 172us/step - loss: 0.0627 - acc: 0.9817 - val_loss: 0.1789 - val_acc: 0.9470\n",
      "Epoch 6/20\n",
      "6000/6000 [==============================] - 1s 156us/step - loss: 0.0457 - acc: 0.9862 - val_loss: 0.1875 - val_acc: 0.9469\n",
      "Epoch 7/20\n",
      "6000/6000 [==============================] - 1s 175us/step - loss: 0.0318 - acc: 0.9913 - val_loss: 0.2369 - val_acc: 0.9342\n",
      "Epoch 8/20\n",
      "6000/6000 [==============================] - 1s 167us/step - loss: 0.0191 - acc: 0.9945 - val_loss: 0.1935 - val_acc: 0.9479\n",
      "Epoch 9/20\n",
      "6000/6000 [==============================] - 1s 200us/step - loss: 0.0166 - acc: 0.9950 - val_loss: 0.2157 - val_acc: 0.9487\n",
      "Epoch 10/20\n",
      "6000/6000 [==============================] - 1s 183us/step - loss: 0.0103 - acc: 0.9968 - val_loss: 0.2240 - val_acc: 0.9487\n",
      "Epoch 11/20\n",
      "6000/6000 [==============================] - 1s 181us/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.2351 - val_acc: 0.9495\n",
      "Epoch 12/20\n",
      "6000/6000 [==============================] - 1s 156us/step - loss: 0.0119 - acc: 0.9978 - val_loss: 0.2193 - val_acc: 0.9527\n",
      "Epoch 13/20\n",
      "6000/6000 [==============================] - 1s 160us/step - loss: 0.0056 - acc: 0.9987 - val_loss: 0.2972 - val_acc: 0.9401\n",
      "Epoch 14/20\n",
      "6000/6000 [==============================] - 1s 159us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.2402 - val_acc: 0.9522\n",
      "Epoch 15/20\n",
      "6000/6000 [==============================] - 1s 165us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.2469 - val_acc: 0.9526\n",
      "Epoch 16/20\n",
      "6000/6000 [==============================] - 1s 163us/step - loss: 1.3171e-04 - acc: 1.0000 - val_loss: 0.2510 - val_acc: 0.9538\n",
      "Epoch 17/20\n",
      "6000/6000 [==============================] - 1s 152us/step - loss: 0.0117 - acc: 0.9972 - val_loss: 0.2634 - val_acc: 0.9531\n",
      "Epoch 18/20\n",
      "6000/6000 [==============================] - 1s 161us/step - loss: 8.1169e-05 - acc: 1.0000 - val_loss: 0.2679 - val_acc: 0.9538\n",
      "Epoch 19/20\n",
      "6000/6000 [==============================] - 1s 160us/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.2906 - val_acc: 0.9513\n",
      "Epoch 20/20\n",
      "6000/6000 [==============================] - 1s 158us/step - loss: 4.8157e-05 - acc: 1.0000 - val_loss: 0.3611 - val_acc: 0.9427\n",
      "Test loss: 0.36114850144078287\n",
      "Test accuracy: 0.9427\n",
      "Training Shape: (60000, 784)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.2162 - acc: 0.9321 - val_loss: 0.0928 - val_acc: 0.9724\n",
      "Epoch 2/20\n",
      " 4224/60000 [=>............................] - ETA: 7s - loss: 0.0943 - acc: 0.9711"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1796b7be0eb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msix_hundred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msix_thousand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msixty_thousand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-e86c56d30b77>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(sample_count)\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                         validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/neural-mnist/env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/git/neural-mnist/env/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/neural-mnist/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/neural-mnist/env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/neural-mnist/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log10_histories = [test_model(60), test_model(600), test_model(6000), test_model(60000)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Bar Graphs\n",
    "plt.figure(0)\n",
    "plt.title('Training Data Size vs. Training Accuracy')\n",
    "plt.ylabel('Training Accuracy %')\n",
    "plt.grid(axis='y', linestyle='dashed')\n",
    "bar_titles = ('60 Samples', '600 Samples', '6,000 Samples', '60,000 Samples')\n",
    "accuracy = [history['acc'][-1] for history in log10_histories]\n",
    "x_nums = np.arange(len(bar_titles))\n",
    "plt.bar(x_nums, accuracy, align='center')\n",
    "\n",
    "plt.xticks(x_nums, bar_titles)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('Training Data Size vs. Validation Accuracy')\n",
    "plt.ylabel('Validation Accuracy %')\n",
    "plt.grid(axis='y', linestyle='dashed')\n",
    "bar_titles = ('60 Samples', '600 Samples', '6,000 Samples', '60,000 Samples')\n",
    "accuracy = [history['val_acc'][-1] for history in log10_histories]\n",
    "x_nums = np.arange(len(bar_titles))\n",
    "plt.bar(x_nums, accuracy, align='center')\n",
    "plt.xticks(x_nums, bar_titles)\n",
    "\n",
    "# Line graphs\n",
    "x_data = np.arange(0, epochs+1)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.title(\"Training Accuracy Vs. Epoch\")\n",
    "plt.ylabel('Training Accuracy %')  \n",
    "plt.xlabel('Epoch')\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.xlim(1, epochs)\n",
    "plt.xticks(np.arange(0, epochs+1, step=2.0))\n",
    "for history in log10_histories:\n",
    "    plt.plot(x_data, history['acc'])\n",
    "plt.legend(['60 Samples', '600 Samples', '6,000 Samples', '60,000 Samples'], loc='lower right') \n",
    "\n",
    "plt.figure(3)\n",
    "plt.title(\"Validation Accuracy Vs. Epoch\")\n",
    "plt.ylabel('Validation Accuracy %')  \n",
    "plt.xlabel('Epoch')\n",
    "plt.grid(linestyle='dashed')\n",
    "plt.xlim(1, epochs)\n",
    "plt.xticks(np.arange(0, epochs+1, step=2.0))\n",
    "for history in log10_histories:\n",
    "    plt.plot(x_data, history['val_acc']) \n",
    "plt.legend(['60 Samples', '600 Samples', '6,000 Samples', '60,000 Samples'], loc='lower right') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "neural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
